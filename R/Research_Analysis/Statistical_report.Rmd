---
title: "STAC67 Final Project"
author: "Mingjia Chen, Kuei-Sheng Hou, Raymond Moy, Ruichen Rachel Zhou"
date: '2023-04-10'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
```

```{r, include=FALSE}
# 
#libraries
library(lmtest)
library("readxl")
library(dplyr)
library(leaps)
library("ggplot2")                     
library("GGally")
library(readxl)
library(tidyverse)
library(dplyr)
library(knitr)
library(xtable)
library(olsrr)
library(ggpubr)
library(corrplot)
#library("MASS")
```

 Mingjia Chen: Evaluate influential and outlying observations, Abstract, Conclusion, and formatting final report and slides 
 
 
 Kuei-Sheng Hou: Data Cleaning, checked Multicollinearity before model selection, conduct model selection, and tailored the final report.
 
 
 Raymond Moy: Examining correlations in data, checking LINE assumptions, model diagnostics and remedial measures.
 
 
 Ruichen Rachel Zhou: Data cleaning and summarizing into tables, creating histograms, checking data validation


\begin{center} 

\textbf{Significant and Influential Factors of Systolic Blood Pressure}\\
\textbf{Group 6} \\

\end{center}


\begin{center} 
\textbf{Library Used in Case Study:} \\

library(dplyr)\\
library("GGally")\\
library("ggplot2")\\
library(ggpubr)\\
library(knitr)\\
library(leaps)\\
library(lmtest)\\
library(olsrr)\\
library("readxl")\\
library(tidyverse)\\
library(xtable)\\
library(corrplot)\\
library("MASS")\\


\end{center}

 
\pagebreak

# Background and Significance

## Abstact

Systolic blood pressure is a medical measure that indicates how much pressure blood is exerting against artery walls when the heart beats. The measure is considered as a major risk factor for cardiovascular disease for people over fifty-five years old (American Heart Association, 2023). There may be numerous internal and external factors influencing the volume or level of systolic blood pressure. From our analysis, we have found these significant factors: smoking status, exercise level, height, alcohol use, treatment status, body mass index (BMI), interaction between smoke status and alcohol, and interaction between treatment and BMI.

This case study aims to better understand the relationship between various factors and the blood pressure by speculating and analyzing possible key predicting factors.

The research question is: What Factors Play Significant Role in Influencing Systolic Blood Pressure?

## Variable description

- Gender: gender of the participant (Female = F, Male = M)
- Marital status: (Married = Y, Not Married = N)
- Smoking status: (Smoker = Y, Non-Smoker = N)
- Age: Age of the participants in years
- Weight: in lbs
- Height: in inches
- Body Mass Index (BMI) = (weight/height^2) * 703
- Overweight: Normal = 1, Overweight = 2, Obese = 3
- Race: 1, 2, 3, or 4 (Categorical)
- Exercise level: Low = 1, Medium = 2, High = 3
- Alcohol Use: Low = 1, Medium = 2, High = 3
- Stress level: Low = 1, Medium = 2, High = 3
- Salt (NaCl) Intake Level: Low = 1, Medium = 2, High = 3
- Childbearing Potential:  Male = 1, Able Female = 2, Unable Female = 3
- Income Level: Low = 1, Medium = 2, High = 3
- Education Level:  Low = 1, Medium = 2, High = 3
- Treatment (for hypertension): Treated = Y, Untreated = N
- Systolic Blood Pressure (SBP): continuous measure

```{r echo=FALSE}
set.seed(1004882941)
data <- read_excel("BloodPressure.xlsx")
```

```{r echo=FALSE}
## Label data
# We will only showcase part of labeling for length of the report

data$overwt[data$overwt == 1] = "Normal"
data$overwt[data$overwt == 2] = "Overweight"
data$overwt[data$overwt == 3] = "Obese"
```

```{r, include=FALSE}
data$exercise[data$exercise == 1] = "Low"
data$exercise[data$exercise == 2] = "Medium"
data$exercise[data$exercise == 3] = "High"

data$alcohol[data$alcohol == 1] = "Low"
data$alcohol[data$alcohol == 2] = "Medium"
data$alcohol[data$alcohol == 3] = "High"

data$stress[data$stress == 1] = "Low"
data$stress[data$stress == 2] = "Medium"
data$stress[data$stress == 3] = "High"

data$salt[data$salt == 1] = "Low"
data$salt[data$salt == 2] = "Medium"
data$salt[data$salt == 3] = "High"

data$chldbear[data$chldbear == 1] = "Low"
data$chldbear[data$chldbear == 2] = "Medium"
data$chldbear[data$chldbear == 3] = "High"

data$income[data$income == 1] = "Low"
data$income[data$income == 2] = "Medium"
data$income[data$income == 3] = "High"

data$educatn[data$educatn == 1] = "Low"
data$educatn[data$educatn == 2] = "Medium"
data$educatn[data$educatn == 3] = "High"

data$trt[data$trt == 1] = "Treated"
data$trt[data$trt == 0] = "Untreated"
```

## Split data 
Let's use 70% of dataset as training set and 30% as testing set
```{r}
sample = sample(seq_len(nrow(data)), size = floor(0.7 * nrow(data)), replace = FALSE)
data.train  <- data[sample, ]
data.test   <- data[-sample, ]
```

# Exploratory Data Analysis / Data visualization

## MultiCollinearity
```{r , message=FALSE, include=FALSE}
fit <- lm(sbp~ gender + married + smoke + exercise + age + weight + height + overwt + factor(race) 
          + alcohol + trt + bmi + stress + salt + chldbear + income + educatn, data = data.train)

fit_simple <- lm(sbp~ 1, data = data.train)
data.numeric = select(data.train, sbp, age, weight, height, bmi)
cor(data.numeric)
```

```{r echo=FALSE, fig.align="center", , message=FALSE, out.width='60%'}
predictors.numeric = select(data, age, weight, height, bmi) #not including the dependent variable
ggpairs(predictors.numeric)
```

From the correlation table, BMI with weight and height have the highest correlation and possible multicollinearity. We also suspect that weight, height, and BMI have some correlation with categorical variable overweight status. In our final models, we only have height and BMI as those 2 predictors are sufficient in capturing the data of these 4 related variables. Additionally, BMI is a linear function of weight, but a non-linear function of height. Taking the height instead of weight predictor helps us avoid multicollinearity as much as possible.

## Find best model
```{r, results="hide"}
step(fit, direction = "backward")$anova
best_fit_backward = lm(sbp ~ smoke + exercise + height + alcohol + trt + bmi, data = data.train)
```
```{r include=FALSE}
drop1(lm(sbp ~ smoke + exercise + age + height + alcohol + trt + bmi, data = data.train),
      test = "Chisq")
anova(lm(sbp ~ smoke + exercise + height + alcohol + trt + bmi, data = data.train),
      lm(sbp ~ smoke + exercise + age + height + alcohol + trt + bmi, data = data.train),
      test = "Chisq")
```
After testing, best model for backward direction is sbp ~ smoke + exercise + height + alcohol + trt + bmi
AIC = 2288.4

```{r,results="hide"}
step(fit_simple, scope=list(upper = fit, lower = fit_simple),direction = "forward")$anova
best_fit_forward = lm(formula = sbp ~ bmi + exercise + smoke + trt + height + alcohol, 
                      data = data.train)
```
```{r include=FALSE}
drop1(lm(sbp ~ bmi + trt + smoke + exercise + alcohol + height + age, data = data.train),
      test = "Chisq")
anova(lm(sbp ~ bmi + trt + smoke + exercise + alcohol + height, data = data.train),
      lm(sbp ~ bmi + trt + smoke + exercise + alcohol + height + age, data = data.train),
      test = "Chisq")
```

After testing, best model for forward direction is sbp ~ bmi + exercise + smoke + trt + height + alcohol, which is the same model as backward direction. AIC = 2288.4

## Check interaction in both direction
```{r, results="hide"}
fit_inter <- lm(sbp ~ (smoke*exercise*height*alcohol*trt*bmi), data = data.train)
step(fit_inter, direction = "backward")$anova
best_fit_inter_backward = lm(sbp ~ smoke + exercise + height + alcohol + trt + 
                               bmi + smoke:alcohol + trt:bmi, data = data.train)
```
```{r include=FALSE}
anova(lm(sbp ~ smoke + exercise + height + alcohol + trt + bmi + smoke:alcohol + trt:bmi, data = data.train),
      lm(sbp ~ smoke + exercise + height + alcohol + trt + bmi + smoke:alcohol + smoke:trt + trt:bmi,
         data = data.train), test = "Chisq")
drop1(lm(sbp ~ smoke + exercise + height + alcohol + trt + bmi + smoke:alcohol + trt:bmi,
         data = data.train), test = "Chisq")

```

AIC = 2277.34  
Best model with interaction term is using backward selection: 
sbp ~ smoke + exercise + height + alcohol + trt + bmi + smoke:alcohol + trt:bmi
    
```{r, results="hide"}
step(fit_simple, scope=list(upper = fit_inter, lower = fit_simple),direction = "forward")$anova
best_fit_inter_forward = lm(sbp ~ bmi + trt + smoke + exercise + alcohol + 
                              height + bmi:trt + smoke:alcohol, data = data.train)
```
AIC=2277.34  
Best model with interaction term is using forward selection:
sbp ~ bmi + trt + smoke + exercise + alcohol + height + bmi:trt + smoke:alcohol
which is the same model as backward selection

## Test models with interaction and without
```{r,out.width = '60%',fig.align="center"}
anova(best_fit_backward, best_fit_inter_backward, test = "Chisq")
best_model = lm(sbp ~ bmi + trt + smoke + exercise + alcohol + height + 
                  bmi:trt + smoke:alcohol, data = data.train)
```
from the anova test, the complex model is better
best_model = sbp ~ smoke + exercise + height + alcohol + trt + bmi + smoke:alcohol + trt:bmi


# Checking LINE assumptions
```{r include=FALSE}
resids = best_model$residuals

attach(mtcars)
par(mfrow=c(2,3))

plot(best_model)
#resid vs fitted seems okay, very clustered around 0 tho
#QQplot looking very good. some minor outliers, around the ends
#2 outlying leaverage points

hist(resids / 25.64, main="semi-studentized residual histogram", xlab="semi-studentized residuals", probability = TRUE)
#add the normal line on top
histx <- rnorm(200)
x2 <- seq(min(histx), max(histx), length = 40)
# Normal curve
fun <- dnorm(x2, mean = mean(histx), sd = sd(histx))
lines(x2, fun, col = 2, lwd = 2)
#looks vaguely normal, skewed to the left

b = boxplot(resids)
#one very apparent outlier

shapiro.test(resids)
#p-value is > 0.05 we fail to reject the null hypothesis, would indicate that the resids are normal

bptest(best_model)
#reject the null hypothesis that the resid variances are equal
#we have evidence that the residual variances are NOT equal!
```

- Residuals vs fitted plot: A band around 0 indicates the linearity assumption holds. The points also seem randomly scattered, suggesting the independent errors assumption also holds.
- QQplot is mostly aligned with the expected normal distribution. There are some outliers at the tails.
- 2 outlying leverage points
- histogram hist of semi-studentised resids looks vaguely normal, slightly skewed to the left
- boxplot shows one very apparent outlier
- Shapiroâ€“Wilk test: p-value is > 0.05 we fail to reject the null hypothesis, would indicate that the resids are normal
- Scale-Location plot line is very bent at for fitted values below 150. Shows that variances of errors are not equal.
- Breusch-Pagan Test: reject the null hypothesis that the resid variances are equal
we have evidence that the residual variances are NOT equal! Thus, we make a weighted least squares model.

We also tried to create the boxcox transformed model, but that did not make the variances of errors equal.


## Weighted least squares
```{r, include=FALSE}
error.fitted1 = lm(abs(best_model$residuals) ~ best_model$fitted.values)
weights1 = 1 / (error.fitted1$fitted.values)^2
WLS1 = lm(sbp ~ bmi + exercise + smoke + trt + height + alcohol + bmi:trt + 
            bmi:exercise, data = data.train, weights = weights1) #has better R^2 values
shapiro.test(WLS1$fitted.values)
bptest(WLS1)
```
```{r, echo=FALSE,message=FALSE,fig.align="center", warning=FALSE, out.width='70%'}
attach(mtcars)
par(mfrow=c(2,3))
plot(WLS1)
boxplot(WLS1$residuals, ylab = "residual", title="Residual Boxplot")
hist(WLS1$residuals, main="residual histogram", xlab="residuals")
```

The weighted least squares model passed the BP-test and has somewhat more convincing model diagnostic plots than the OLS model.

## MultiCollinearity 
Correlation Matrix:
```{r echo=FALSE, fig.align="center", warning=FALSE, out.width='25%'}
data.final.numeric = select(data.train, sbp, height, bmi)
corrplot(cor(data.final.numeric))
```

```{r, include=FALSE}
data <- read_excel("BloodPressure.xlsx")
glimpse(data)
clean_data <- data %>%
  mutate(male = as.numeric(gender=="M"), smoking = as.numeric(smoke=="Y")) %>%
  dplyr:::select(sbp, male, smoking, exercise, age, weight, height, alcohol, trt, bmi)
glimpse(clean_data)
```

```{r echo=FALSE, fig.align="center", message=FALSE, warning=FALSE, out.width='60%'}
attach(mtcars)
par(mfrow=c(2,3))
hist(clean_data$sbp, main = "Histogram of Systolic Blood Pressure(SBP)")
hist(clean_data$age, main = "Histogram of Age")
hist(clean_data$weight, main = "Histogram of Weight")
hist(clean_data$height, main = "Histogram of Height")
hist(clean_data$bmi, main = "Histogram of BMI")
hist(clean_data$alcohol, main = "Historgram of Alcohol")
```

Graph for SBP-> slightly skewed, closest to normal distribution. No particular observation can be made for the remaining graphs.

```{r, include = FALSE}
summary(clean_data$sbp)
sd(clean_data$sbp)

summary(clean_data$age)
sd(clean_data$age)

summary(clean_data$weight)
sd(clean_data$weight)

summary(clean_data$height)
sd(clean_data$height)

summary(clean_data$bmi)
sd(clean_data$bmi)

sum(clean_data$male==1)
sum(clean_data$male==0)

sum(clean_data$smoking==1)
sum(clean_data$smoking==0)

sum(clean_data$exercise==1)
sum(clean_data$exercise==2)
sum(clean_data$exercise==3)

sum(clean_data$alcohol==1)
sum(clean_data$alcohol==2)
sum(clean_data$alcohol==3)

sum(clean_data$trt==1)
sum(clean_data$trt==0)

```
By calculating the summary and sd from cleaned data, we obtain two summary tables:

| Variables | Descriptions               | Min | Q1  | Median | Mean | Q3  | Max | SD |
|-----------|----------------------------|-----|-----|--------|------|-----|-----|----|
|sbp        |Systolic Blood Pressure(SBP)| 67  | 130 | 140.5  | 145  |162.2| 224 | 28 |
|age        |years                       | 18  | 28  |  40    |  40  | 52  | 64  |13.3|
|weight     |lbs                         | 90  | 133 |  168   |166.6 | 198 | 249 |40.9|
|height     |inches                      | 54  | 60  |  65    |65.33 | 70  | 77  |6.2 |
|bmi        |Body Mass Index(BMI)        | 11  | 21  |  27    |27.66 | 33  | 53  |8.6 |
  

| Variables | Categories                 | n   |
|-----------|----------------------------|-----|
|Gender     |Female                      | 264 | 
|           |Male                        | 236 | 
|Smoking    |yes                         | 266 | 
|           |no                          | 234 | 
|Exercise   |low                         | 195 |
|           |medium                      | 136 |
|           |high                        | 169 |
|Alcohol    |low                         | 160 |
|           |medium                      | 167 |
|           |high                        | 173 |
|Treatment  |yes                         | 101 | 
|           |no                          | 399 |

```{r,results="hide", include=FALSE, r,results="hide"}
cor(clean_data)
```


```{r}
fit1 <- lm(sbp ~ . , data = clean_data)
fit2 <- lm(sbp ~ smoking + exercise + alcohol + trt + bmi, data = clean_data)
full <- lm(sbp ~ . , data = data)
fit.null <- lm(sbp ~ 1, data = data)
model <- step(fit.null,direction = "forward", scope = list("lower" = fit.null, "upper" = full), trace = 0)

# multiple regression model
AICs = c(AIC(fit1),AIC(fit2),AIC(model))
AICs
```
fit1 and fit2 have similar AIC values

# Model Validation / Diagnostics
```{r}
##### Obtain validation sets
set.seed(12345)
n = nrow(clean_data)
cv.samp <- sample(1:n, round(0.5*n),replace = FALSE)
cv.in <- clean_data[cv.samp,]
cv.out <- clean_data[-cv.samp,]

##### fit model for training set
fit.cv.in <- lm(sbp ~ smoking+exercise+alcohol+trt+bmi, data=cv.in)
anova(fit.cv.in)

##### Obtain Predicted  values and prediction errors for validation sample
##### Regression is based on same predictors
##### Compute MSPR
pred.cv.out <- predict(fit.cv.in, cv.out[,c(3,4,8,9,10)])
delta.cv.out <- clean_data[-cv.samp,]-pred.cv.out
n.star = dim(cv.out)[1]
MSPR <- sum(delta.cv.out)^2/n.star
MSPR

#Fit Model on Validation Sample and Compare regression coefficients with model for Training Sample
fit.cv.out <- lm(sbp ~ smoking+exercise+alcohol+trt+bmi, data=cv.out)
anova(fit.cv.out)
```

|           |$\hat{\beta_0}$|$\hat{\beta_1}$|$\hat{\beta_2}$|$\hat{\beta_3}$|$\hat{\beta_4}$|$\hat{\beta_5}$| $R^2_{adj}$ |
|-----------|---------------|---------------|---------------|---------------|---------------|---------------|--------------|
|Training   | 117.05 (8.45) | 10.96 (3.38)  | -5.85 (1.94)  | 4.76 (2.03)   | -15.28 (4.17) | 0.99 (0.21)   | 0.1657       |
|Validation | 114.32 (7.67) | 10.66 (3.21)  | -4.47 (1.89)  | 6.84 (2.00)   | -9.57 (4.03)  | 0.77 (0.18)   | 0.1589       |



Since the regression results for the training and validation data sets are similar, we can conclude that the model is valid and we can make statistical reference based on the model

## Outlying & Influential Points
```{r,message=FALSE}
influences = influence.measures(best_model)
# Y outlying points
best_model = lm(sbp ~ bmi + trt + smoke + exercise + alcohol + height + bmi:trt 
                + smoke:alcohol, data = data)
# studentized deleted residuals
t = rstudent(best_model)
#standardized residuals
r = rstandard(best_model)
rt.table = cbind(r,t)

alpha = 0.05
n = dim(data)[1]
p.prime = length(coef(best_model))
t.crit = qt(1-alpha/(2*n), n - p.prime - 1)
t.crit
```

```{r, results='hide'}
# X outlying points
hii = hatvalues(best_model)
which(hii > 2*p.prime/n)
which(hii > 0.05)
```

```{r,out.width = '50%',fig.align="center",message=FALSE,warning=FALSE}
#Influential observations
DFFITTS = dffits(best_model)
#Cook's Distance
D = cooks.distance(best_model)

DFBETAS = dfbetas(best_model)
c(which(abs(t) > t.crit),which(DFFITTS >1),which(D>qf(0.2,p.prime,n-p.prime)),which(DFBETAS > 1))
```
We got named integer(0) for result, meaning there are no outlier observations of Y according to studentized deleted residual.  

```{r,include=FALSE}
# added variable plots
ols_plot_added_variable(best_model)
ols_plot_dfbetas(best_model)
```

```{r,include=FALSE,warning=FALSE,message=FALSE}
p6 = ols_plot_resid_stud_fit(best_model)
p4 = ols_plot_dffits(best_model)
p5 = ols_plot_resid_lev(best_model)
p1 = ols_plot_cooksd_chart(best_model)
```

```{r,echo=FALSE,warning=FALSE}
ggarrange(p1,p4,p5,p6,ncol=2,nrow=2)
```
According to BFFITS, D and DFBETAS, we do not have influential observations in the data.  
Even though there are outliers of X observations, we decide to keep them as they are. Outliers are not too rare in real life scenarios. As long as there are no influential observations in the model, we will stick to the original X dataset. This may also help us avoid the problem of overfitting.  

# Discussion/Conclusion

We investigated measures that could play significant roles in predicting SBP. From the result of the case study, with a large dataset of 500 observations, we see the relationship between SBP and numerous contributing factors such as Smoking status, Exercise level, Height, Alcohol use, Treatment, Body Mass Index (BMI) and 2 interactions (smoking:alcohol, treatment:BMI). In conclusion, the study could provide insight on how we could improve our SBP and predict it using these measures that a family doctor could obtain.
Despite of the strength of the study, there are some limitations as well. For instance, where the population dataset was from is unclear, we cannot know how accurate the data is. The study may face challenges representing any population in real life. During the study, outlying observations  of X variables are detected. There could be confounding factors. 
For future directions, new samples could be obtained from different types of populations and cultures. We can also reuse the model for on datasets to see if the model is explaining real life scenarios in general or only specific to this case.

\pagebreak

# Reference
Understanding blood pressure readings. www.heart.org. (2023, February 2). Retrieved April 2, 2023, from https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings 
